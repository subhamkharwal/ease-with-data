{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "976ac579-950a-48ec-8d3f-e6ccd843a601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script to initialize the Data Warehouse\\Lakehouse and create the required tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a409d3d-41a6-47ec-9474-b392da225d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5901d52-b711-4c91-9713-afce7f56f475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-8c71c36c-027a-44cb-8952-070467d32be6;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.1.1 in central\n",
      "\tfound io.delta#delta-storage;2.1.1 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.2 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.1026 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      ":: resolution report :: resolve 218ms :: artifacts dl 12ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.1026 from central in [default]\n",
      "\tio.delta#delta-core_2.12;2.1.1 from central in [default]\n",
      "\tio.delta#delta-storage;2.1.1 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.2 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   7   |   0   |   0   |   0   ||   7   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-8c71c36c-027a-44cb-8952-070467d32be6\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 7 already retrieved (0kB/6ms)\n",
      "23/02/01 15:50:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://46346aae9d54:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Intialize Lakehouse</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ff04c5bca30>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Intialize Lakehouse\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c583868b-c9b4-4180-a3c5-855eea5e2b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "|       dw|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create dw schema in catalog\n",
    "spark.sql(\"create database if not exists dw\");\n",
    "spark.sql(\"show databases\").show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a02bf715-0858-40cd-9f02-f18976b213c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPARK-APP: Store dimension created\n"
     ]
    }
   ],
   "source": [
    "# Create Store Dim table\n",
    "spark.sql(\"\"\"drop table if exists dw.dim_store\"\"\");\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "create table dw.dim_store (\n",
    "    row_wid string,\n",
    "    store_id string,\n",
    "    store_name string,\n",
    "    address string,\n",
    "    city string,\n",
    "    state string,\n",
    "    zip_code string,\n",
    "    phone_number string,\n",
    "    insert_dt date,\n",
    "    update_dt date\n",
    ")\n",
    "USING delta\n",
    ";\n",
    "\"\"\");\n",
    "\n",
    "print(\"SPARK-APP: Store dimension created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "139b02d7-525b-4919-9b89-b43c51b700c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPARK-APP: Plan Type dimension created\n"
     ]
    }
   ],
   "source": [
    "# Create Plan Type Dimension\n",
    "spark.sql(\"\"\"drop table if exists dw.dim_plan_type\"\"\");\n",
    "spark.sql(\"\"\"\n",
    "create table dw.dim_plan_type (\n",
    "    plan_type_code string,\n",
    "    plan_name string,\n",
    "    insert_dt date,\n",
    "    update_dt date\n",
    ")\n",
    "USING delta\n",
    ";\n",
    "\"\"\");\n",
    "\n",
    "print(\"SPARK-APP: Plan Type dimension created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ee98612-192f-4e55-87cc-e87c7b4b7cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPARK-APP: Date dimension created\n"
     ]
    }
   ],
   "source": [
    "# Create Date Dimension\n",
    "spark.sql(\"\"\"drop table if exists dw.dim_date\"\"\");\n",
    "spark.sql(\"\"\"\n",
    "create table dw.dim_date (\n",
    "    row_wid string,\n",
    "    date date,\n",
    "    month int,\n",
    "    year int,\n",
    "    day_of_week string,\n",
    "    insert_dt date,\n",
    "    update_dt date\n",
    ")\n",
    "USING delta\n",
    ";\n",
    "\"\"\");\n",
    "\n",
    "print(\"SPARK-APP: Date dimension created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a62ca64-287f-48b9-9c50-67b43524621d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPARK-APP: Product dimension created\n"
     ]
    }
   ],
   "source": [
    "# Create Product Dimension\n",
    "spark.sql(\"\"\"drop table if exists dw.dim_product\"\"\");\n",
    "spark.sql(\"\"\"\n",
    "create table dw.dim_product (\n",
    "    row_wid string,\n",
    "    product_id string,\n",
    "    product_name string,\n",
    "    brand string,\n",
    "    type string,\n",
    "    flavor string,\n",
    "    size string,\n",
    "    price bigint,\n",
    "    expiration_dt date,\n",
    "    image_url string,\n",
    "    effective_start_dt date,\n",
    "    effective_end_dt date,\n",
    "    active_flg int,\n",
    "    insert_dt date,\n",
    "    update_dt date\n",
    ")\n",
    "USING delta\n",
    ";\n",
    "\"\"\");\n",
    "\n",
    "print(\"SPARK-APP: Product dimension created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1adc179-6965-4c55-9d85-e86397b8d7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPARK-APP: Customer dimension created\n"
     ]
    }
   ],
   "source": [
    "# Create Customer Dimension\n",
    "spark.sql(\"\"\"drop table if exists dw.dim_customer\"\"\");\n",
    "spark.sql(\"\"\"\n",
    "create table dw.dim_customer (\n",
    "    row_wid string,\n",
    "    customer_id string,\n",
    "    first_name string,\n",
    "    last_name string,\n",
    "    address string,\n",
    "    city string,\n",
    "    state string,\n",
    "    zip_code string,\n",
    "    phone_number string,\n",
    "    email string,\n",
    "    date_of_birth date,\n",
    "    plan_type string,\n",
    "    effective_start_dt date,\n",
    "    effective_end_dt date,\n",
    "    active_flg int,\n",
    "    insert_dt date,\n",
    "    update_dt date\n",
    ")\n",
    "USING delta\n",
    ";\n",
    "\"\"\");\n",
    "\n",
    "print(\"SPARK-APP: Customer dimension created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77c8a620-ab75-457f-a893-f758c3eda687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPARK-APP: Sales Fact created\n"
     ]
    }
   ],
   "source": [
    "# Create Sales Fact\n",
    "spark.sql(\"\"\"drop table if exists dw.fact_sales\"\"\");\n",
    "spark.sql(\"\"\"\n",
    "create table dw.fact_sales (\n",
    "    date_wid string,\n",
    "    product_wid string,\n",
    "    store_wid string,\n",
    "    customer_wid string,\n",
    "    order_id string,\n",
    "    invoice_num string,\n",
    "    qty int,\n",
    "    tax double,\n",
    "    discount double,\n",
    "    line_total double,\n",
    "    integration_key string,\n",
    "    insert_dt date\n",
    ")\n",
    "USING delta\n",
    ";\n",
    "\"\"\");\n",
    "\n",
    "print(\"SPARK-APP: Sales Fact created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5cd652a-5d06-40d9-bd20-22051330fca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+-----------+\n",
      "|namespace|    tableName|isTemporary|\n",
      "+---------+-------------+-----------+\n",
      "|       dw| dim_customer|      false|\n",
      "|       dw|     dim_date|      false|\n",
      "|       dw|dim_plan_type|      false|\n",
      "|       dw|  dim_product|      false|\n",
      "|       dw|    dim_store|      false|\n",
      "|       dw|   fact_sales|      false|\n",
      "+---------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Log all tables in Data Warehouse/Lakehouse\n",
    "\n",
    "spark.sql(\"show tables in dw\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac5f2976-7182-44d7-a309-e375dc7bf7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c432dcdf-5418-49e8-8fb0-c23a4eac8b7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
