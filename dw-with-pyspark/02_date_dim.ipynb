{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54663222-aed8-463d-9bb6-7d3b23c32f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script to create date for Time Dimesion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8d350312-8586-4023-ac2d-7f9f5bce2118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_date, expr, current_date, date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a52bd82d-00e8-47a9-b7a2-6b00c14ec99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date Source data generating function for next 1 year\n",
    "def _date_dim(start_run_dt: str = '20230101') -> list:\n",
    "    _data = []\n",
    "    _start_date = datetime.strptime(start_run_dt, '%Y%m%d')\n",
    "    _data.append([\n",
    "        datetime.strftime(_start_date, '%Y-%m-%d'), \n",
    "        datetime.strftime(_start_date, '%d'),\n",
    "        datetime.strftime(_start_date, '%m'),\n",
    "        datetime.strftime(_start_date, '%Y'),\n",
    "        datetime.strftime(_start_date, '%A')])\n",
    "    _next_date = _start_date\n",
    "    for i in range(0, 364):\n",
    "        _next_date = _next_date + timedelta(days = 1)\n",
    "        _data.append([\n",
    "        datetime.strftime(_next_date, '%Y-%m-%d'), \n",
    "        datetime.strftime(_next_date, '%d'),\n",
    "        datetime.strftime(_next_date, '%m'),\n",
    "        datetime.strftime(_next_date, '%Y'),\n",
    "        datetime.strftime(_next_date, '%A')])\n",
    "        \n",
    "    return _data\n",
    "\n",
    "# Column for Date Source\n",
    "_cols = ['date', 'day', 'month', 'year', 'day_of_week']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2515365e-5d08-45de-829f-7a052bcc2734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-d61a3196-3945-4e66-b445-6d7e0d4e9b55;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.1.1 in central\n",
      "\tfound io.delta#delta-storage;2.1.1 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.2 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.1026 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      ":: resolution report :: resolve 237ms :: artifacts dl 13ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.1026 from central in [default]\n",
      "\tio.delta#delta-core_2.12;2.1.1 from central in [default]\n",
      "\tio.delta#delta-storage;2.1.1 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.2 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   7   |   0   |   0   |   0   ||   7   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-d61a3196-3945-4e66-b445-6d7e0d4e9b55\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 7 already retrieved (0kB/9ms)\n",
      "23/02/02 10:51:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session Created, webui: http://46346aae9d54:4040\n"
     ]
    }
   ],
   "source": [
    "# Generate the Spark Session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Date DIM Load\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark Session Created, webui: \" + spark.sparkContext.uiWebUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1cfc058b-c2c6-4aca-82df-d1a63f619000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the source dataframe\n",
    "df_landing = spark.createDataFrame(data=_date_dim(), schema=_cols)\n",
    "\n",
    "# Transform columns\n",
    "df_stage = df_landing \\\n",
    "    .withColumn(\"date\", to_date(\"date\", \"yyyy-MM-dd\")) \\\n",
    "    .withColumn(\"day\", expr(\"cast(day as int)\")) \\\n",
    "    .withColumn(\"month\", expr(\"cast(month as int)\")) \\\n",
    "    .withColumn(\"year\", expr(\"cast(year as int)\")) \\\n",
    " \n",
    "# Create final dim df\n",
    "df_dim = df_stage \\\n",
    "    .withColumn(\"row_wid\", date_format(\"date\", \"yyyyMMdd\")) \\\n",
    "    .withColumn(\"insert_dt\", current_date()) \\\n",
    "    .withColumn(\"update_dt\", current_date()) \\\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "88759e7a-1ef0-4e02-a4f4-f629ea0c7570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPARK_APP: Writing data to DIM_DATE in APPEND MODE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Write data in DIM DATE\n",
    "print(\"SPARK_APP: Writing data to DIM_DATE in APPEND MODE\")\n",
    "\n",
    "df_dim.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .saveAsTable(\"dw.dim_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "971ea881-1d3c-4c9a-b495-7eb5c5906c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|dim_date_count|\n",
      "+--------------+\n",
      "|           365|\n",
      "+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(1) as dim_date_count from dw.dim_date\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b3b05da0-1da7-4e62-ba6d-1d78257641ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPARK_APP: Dimension Date Loaded Successfully\n"
     ]
    }
   ],
   "source": [
    "print(\"SPARK_APP: Dimension Date Loaded Successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5bb195d4-db93-454d-91cd-e19c22b3edd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
